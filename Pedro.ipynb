{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d747d1a4-8886-4542-9a9b-7a688ce24204",
   "metadata": {},
   "source": [
    "# Pedro - Short Queeze Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25650702-99fc-488f-b9dc-320ac574371e",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6fc2c9-97c4-4fd6-8bd8-2900aa31ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are importing the necessary libraries for the project.\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import alpaca_trade_api as tradeapi\n",
    "import pandas_market_calendars as mcal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac59c7f-3cec-4441-b3d4-9757fbde0189",
   "metadata": {},
   "source": [
    "### Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2556d008-af0c-48e1-8b26-5ddf9a107291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from a .env file.\n",
    "load_dotenv('alpaca.env')\n",
    "\n",
    "# Set Alpaca API key and secret\n",
    "alpaca_api_key = os.getenv(\"ALPACA_API_KEY\")\n",
    "alpaca_secret_key = os.getenv(\"ALPACA_SECRET_KEY\")\n",
    "\n",
    "# Initialize Alpaca API\n",
    "api = tradeapi.REST(\n",
    "    alpaca_api_key,\n",
    "    alpaca_secret_key,\n",
    "    api_version=\"v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f87b9a-5895-41ac-8484-d83d60b3c992",
   "metadata": {},
   "source": [
    "### Short Interest Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d8f33b-19df-4275-be14-0fdd94dc6ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are loading and preprocessing short interest data.\n",
    "\n",
    "short_df = pd.read_csv(Path(\"Resources/ShortFloat.csv\"))\n",
    "short_df.rename(columns={'ShortSqueeze.com Short Interest Data': 'Company Name'}, inplace=True)\n",
    "\n",
    "# Dropping irrelevant columns\n",
    "columns_to_drop = [\n",
    "    'Total Short Interest', 'Days to Cover', 'Performance (52-wk)', 'Short: Prior Mo', '% Change Mo/Mo',\n",
    "    'Shares: Float', 'Avg. Daily Vol.', 'Shares: Outstanding', 'Short Squeeze Rankingâ„¢', '% from 52-wk High',\n",
    "    '(abs)', '% from 200 day MA', '(abs).1', '% from 50 day MA', '(abs).2', '% Insider Ownership',\n",
    "    '% Institutional Ownership'\n",
    "]\n",
    "columns_to_drop = [col for col in columns_to_drop if col in short_df.columns]\n",
    "short_df.drop(columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Converting 'Short % of Float' and 'Market Cap' to numeric and applying filters\n",
    "short_df['Short % of Float'] = pd.to_numeric(short_df['Short % of Float'], errors='coerce')\n",
    "short_df = short_df[short_df['Short % of Float'] >= 17]\n",
    "short_df['Market Cap'] = pd.to_numeric(short_df['Market Cap'], errors='coerce')\n",
    "short_df = short_df[short_df['Market Cap'] >= 300000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8233714c-5ff6-4c85-83a9-aadbede694bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14f546b-0487-4b88-9511-059fa6c58620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Date Cleaning and Mapping\n",
    "# Convert 'Record Date' column to datetime, sort the dataframe by 'Record Date'.\n",
    "date_mapping = {\n",
    "    'JanA': '01-11', 'JanB': '01-25',\n",
    "    'FebA': '02-09', 'FebB': '02-27',\n",
    "    'MarA': '03-09', 'MarB': '03-24',\n",
    "    'AprA': '04-12', 'AprB': '04-25',\n",
    "    'MayA': '05-09', 'MayB': '05-24',\n",
    "    'JunA': '06-09', 'JunB': '06-27',\n",
    "    'JulA': '07-12', 'JulB': '07-25',\n",
    "    'AugA': '08-09', 'AugB': '08-24',\n",
    "    'SepA': '09-12', 'SepB': '09-26',\n",
    "    'OctA': '10-10', 'OctB': '10-24',\n",
    "    'NovA': '11-09', 'NovB': '11-27',\n",
    "    'DecA': '12-11', 'DecB': '12-27',\n",
    "}\n",
    "short_df['Record Date'] = pd.to_datetime(short_df['Record Date'].str.replace(r'(\\d{4})-(\\w+)', lambda m: f'{m.group(1)}-{date_mapping[m.group(2)]}'))\n",
    "short_df.sort_values('Record Date', inplace=True)\n",
    "short_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8935b9ee-e078-40be-8e66-1c65a12baf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3a8602-f1fe-44c5-b593-acb5241ad935",
   "metadata": {},
   "source": [
    "### Insider Trading Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a79556b-9b55-4ec0-8c9c-705d16a9f8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load insider trading data, remove dollar symbols and convert 'Total Amount' and 'Share Price' to numeric.\n",
    "\n",
    "insider_df = pd.read_csv(\"Resources/InsiderTrading.csv\")\n",
    "insider_df['Total Amount'] = insider_df['Total Amount'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n",
    "insider_df['Share Price'] = insider_df['Share Price'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n",
    "insider_df['Date'] = pd.to_datetime(insider_df['Date'])\n",
    "insider_df['Total Amount'] = pd.to_numeric(insider_df['Total Amount'], errors='coerce')\n",
    "insider_df = insider_df[insider_df['Total Amount'] >= 1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ba8669-9129-4601-a944-386675f4ac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "insider_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6cb797-bf50-4ff9-8e2a-405178203cf9",
   "metadata": {},
   "source": [
    "### Data Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628db501-f3fb-4b60-b9b3-fa8c2406c4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge short_df and insider_df based on the Symbol column, rename 'Share Price' to 'Close Price', select necessary columns\n",
    "\n",
    "merged_df = pd.merge(short_df, insider_df, on='Symbol')\n",
    "merged_df['Share Price'] = merged_df['Share Price'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n",
    "merged_df.rename(columns={'Share Price': 'Close Price'}, inplace=True)\n",
    "merged_df = merged_df[['Symbol', 'Short % of Float', 'Total Amount', 'Record Date', 'Close Price', 'Company Name', 'Sector', 'Industry', 'Date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9503889-1b9e-4b54-a1a9-37c0a1534fb2",
   "metadata": {},
   "source": [
    "### Data Cleaning and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e255c0e1-9461-4465-a4fe-b442bae0dc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference between 'Date' and 'Record Date' for each row, filter out rows where 'Date_diff' is more than 30, and drop unnecessary columns.\n",
    "\n",
    "merged_df['Date_diff'] = (merged_df['Date'] - merged_df['Record Date']).dt.days\n",
    "merged_df = merged_df[merged_df['Date_diff'] >= 0]\n",
    "merged_df.sort_values(['Symbol', 'Date_diff'], inplace=True)\n",
    "merged_df.drop_duplicates(subset='Symbol', keep='first', inplace=True)\n",
    "merged_df = merged_df[merged_df['Date_diff'] <= 30]\n",
    "merged_df.drop(columns=['Record Date', 'Date_diff'], inplace=True)\n",
    "\n",
    "# Reorder columns\n",
    "new_column_order = ['Symbol', 'Short % of Float', 'Total Amount', 'Date', 'Close Price', 'Company Name', 'Sector', 'Industry']\n",
    "merged_df = merged_df[new_column_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9903dc37-6273-47ff-8fd5-1a341ac685b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624acc4a-ec8d-4f75-8603-d35b4795a22e",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832b0961-99a7-41cc-99f8-e3531729d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns for Close Prices at future dates and calculate Returns.\n",
    "\n",
    "nyse = mcal.get_calendar('NYSE')\n",
    "desired_days = [2, 3, 4, 5, 7]\n",
    "for day in desired_days:\n",
    "    merged_df[f'Close Price Day {day}'] = np.nan\n",
    "\n",
    "for idx, row in merged_df.iterrows():\n",
    "    trading_days = nyse.valid_days(start_date=row['Date'], end_date=row['Date'] + pd.DateOffset(days=10))\n",
    "\n",
    "    for day in desired_days:\n",
    "        if day <= len(trading_days):\n",
    "            data = yf.download(row['Symbol'], start=trading_days[day - 1], end=trading_days[day - 1] + pd.DateOffset(days=1))\n",
    "            if not data.empty:  \n",
    "                merged_df.loc[idx, f'Close Price Day {day}'] = data['Close'][0] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413c773f-3f1c-4f58-b062-552482a7b485",
   "metadata": {},
   "source": [
    "### Calculate Returns and Highest Day Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fca0c28-2ca6-4052-87cb-83672571855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for day in [5, 7]: \n",
    "    merged_df[f'Return ({day} Days)'] = ((merged_df[f'Close Price Day {day}'] - merged_df['Close Price']) / merged_df['Close Price']) * 100\n",
    "\n",
    "merged_df['Highest Day Return'] = merged_df[[f'Return ({day} Days)' for day in [5, 7]]].max(axis=1)\n",
    "merged_df['Highest Close Price'] = merged_df[[f'Close Price Day {day}' for day in desired_days]].max(axis=1)\n",
    "\n",
    "# Format the 'Close Price' and 'Return' columns to have 2 decimal places\n",
    "for col in merged_df.columns:\n",
    "    if 'Close Price' in col or 'Return' in col:\n",
    "        merged_df[col] = merged_df[col].round(2)\n",
    "\n",
    "\n",
    "# Drop NaNs and reset the index\n",
    "merged_df.dropna(inplace=True)\n",
    "merged_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d2cac3-ac79-48c2-b9f2-73ed0c4c1846",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
