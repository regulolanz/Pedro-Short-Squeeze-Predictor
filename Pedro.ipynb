{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d747d1a4-8886-4542-9a9b-7a688ce24204",
   "metadata": {},
   "source": [
    "# Pedro - Short Queeze Predictor\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b61898-777a-4387-a889-cdf838a3f7ed",
   "metadata": {},
   "source": [
    "### 1. Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dc6fc2c9-97c4-4fd6-8bd8-2900aa31ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from pathlib import Path\n",
    "import pandas_market_calendars as mcal\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, balanced_accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "import hvplot.pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f87b9a-5895-41ac-8484-d83d60b3c992",
   "metadata": {},
   "source": [
    "### 2. Data Preparation Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dc5c0e3a-b298-447d-8f98-e5da85bcb2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHORT_INTEREST_FLOAT_FILTER = 17\n",
    "MARKET_CAP_FILTER = 300000000\n",
    "TOTAL_AMOUNT_FILTER = 1000000\n",
    "DESIRED_DAYS = [1, 2, 5, 7]\n",
    "short_float_filepath = \"Resources/ShortFloat.csv\"\n",
    "insider_trading_filepath = \"Resources/InsiderTrading.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f498dce6-bef6-4cc3-b2a8-b26438528f8d",
   "metadata": {},
   "source": [
    "### 3. Data Loading and Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "45d8f33b-19df-4275-be14-0fdd94dc6ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(short_float_filepath, insider_trading_filepath):\n",
    "    \n",
    "    short_df = pd.read_csv(short_float_filepath)\n",
    "    short_df.rename(columns={'ShortSqueeze.com Short Interest Data': 'Company Name'}, inplace=True)\n",
    "    # Dropping irrelevant columns\n",
    "    columns_to_drop = [\n",
    "    'Total Short Interest', 'Days to Cover', 'Performance (52-wk)', 'Short: Prior Mo', '% Change Mo/Mo',\n",
    "    'Shares: Float', 'Avg. Daily Vol.', 'Shares: Outstanding', 'Short Squeeze Rankingâ„¢', '% from 52-wk High',\n",
    "    '(abs)', '% from 200 day MA', '(abs).1', '% from 50 day MA', '(abs).2', '% Insider Ownership',\n",
    "    '% Institutional Ownership'\n",
    "    ]\n",
    "    columns_to_drop = [col for col in columns_to_drop if col in short_df.columns]\n",
    "    short_df.drop(columns_to_drop, axis=1, inplace=True)\n",
    "    # Convert 'Short % of Float' and 'Market Cap' to numeric and apply filters\n",
    "    short_df['Short % of Float'] = pd.to_numeric(short_df['Short % of Float'], errors='coerce')\n",
    "    short_df = short_df[short_df['Short % of Float'] >= SHORT_INTEREST_FLOAT_FILTER]\n",
    "    short_df['Market Cap'] = pd.to_numeric(short_df['Market Cap'], errors='coerce')\n",
    "    short_df = short_df[short_df['Market Cap'] >= MARKET_CAP_FILTER]\n",
    "    \n",
    "    insider_df = pd.read_csv(insider_trading_filepath)\n",
    "    insider_df['Total Amount'] = insider_df['Total Amount'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n",
    "    insider_df['Share Price'] = insider_df['Share Price'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n",
    "    insider_df['Date'] = pd.to_datetime(insider_df['Date'])\n",
    "    insider_df['Total Amount'] = pd.to_numeric(insider_df['Total Amount'], errors='coerce')\n",
    "    insider_df = insider_df[insider_df['Total Amount'] >= TOTAL_AMOUNT_FILTER]\n",
    "    \n",
    "    return short_df, insider_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b3ec1ac5-7185-4437-b4e9-47b3ff42d15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8b/qhtp32dn1ld9zhxxfhbnshq80000gn/T/ipykernel_70963/1168068969.py:3: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  short_df = pd.read_csv(short_float_filepath)\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "short_df, insider_df = load_and_preprocess_data(short_float_filepath, insider_trading_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375af6ff-e752-4ab8-9c5e-c9d0af0bbcb2",
   "metadata": {},
   "source": [
    "### 4. Feature Engineering Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a14f546b-0487-4b88-9511-059fa6c58620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(short_df, insider_df):\n",
    "    date_mapping = {\n",
    "    'JanA': '01-11', 'JanB': '01-25',\n",
    "    'FebA': '02-09', 'FebB': '02-27',\n",
    "    'MarA': '03-09', 'MarB': '03-24',\n",
    "    'AprA': '04-12', 'AprB': '04-25',\n",
    "    'MayA': '05-09', 'MayB': '05-24',\n",
    "    'JunA': '06-09', 'JunB': '06-27',\n",
    "    'JulA': '07-12', 'JulB': '07-25',\n",
    "    'AugA': '08-09', 'AugB': '08-24',\n",
    "    'SepA': '09-12', 'SepB': '09-26',\n",
    "    'OctA': '10-10', 'OctB': '10-24',\n",
    "    'NovA': '11-09', 'NovB': '11-27',\n",
    "    'DecA': '12-11', 'DecB': '12-27',\n",
    "    }\n",
    "    \n",
    "    short_df['Record Date'] = pd.to_datetime(short_df['Record Date'].str.replace(r'(\\d{4})-(\\w+)', lambda m: f'{m.group(1)}-{date_mapping[m.group(2)]}'))\n",
    "    short_df.sort_values('Record Date', inplace=True)\n",
    "    short_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    merged_df = pd.merge(short_df, insider_df, on='Symbol')\n",
    "    merged_df['Share Price'] = merged_df['Share Price'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n",
    "    merged_df = merged_df[['Symbol', 'Short % of Float', 'Total Amount', 'Record Date', 'Share Price', 'Company Name', 'Sector', 'Industry', 'Date']]\n",
    "    \n",
    "    # Calculate the difference between 'Date' and 'Record Date' for each row\n",
    "    merged_df['Date_diff'] = (merged_df['Date'] - merged_df['Record Date']).dt.days\n",
    "    # Filter out rows where 'Date_diff' is more than 30 and drop unnecessary columns\n",
    "    merged_df = merged_df[merged_df['Date_diff'] >= 0]\n",
    "    merged_df.sort_values(['Symbol', 'Date_diff'], inplace=True)\n",
    "    merged_df.drop_duplicates(subset=['Symbol', 'Date'], keep='first', inplace=True)\n",
    "    merged_df = merged_df[merged_df['Date_diff'] <= 30]\n",
    "    merged_df.drop(columns=['Record Date', 'Date_diff'], inplace=True)\n",
    "\n",
    "    # Reorder columns\n",
    "    new_column_order = ['Symbol', 'Short % of Float', 'Total Amount', 'Date', 'Company Name', 'Sector', 'Industry']\n",
    "    merged_df = merged_df[new_column_order]\n",
    "    \n",
    "    # Create new columns for Close Prices at future dates and calculate Returns.\n",
    "    nyse = mcal.get_calendar('NYSE')\n",
    "    desired_days = DESIRED_DAYS\n",
    "    for day in desired_days:\n",
    "        merged_df[f'Close Price Day {day}'] = np.nan\n",
    "\n",
    "    for idx, row in merged_df.iterrows():\n",
    "        trading_days = nyse.valid_days(start_date=row['Date'], end_date=row['Date'] + pd.DateOffset(days=10))\n",
    "\n",
    "        for day in desired_days:\n",
    "            if day <= len(trading_days):\n",
    "                data = yf.download(row['Symbol'], start=trading_days[day - 1], end=trading_days[day - 1] + pd.DateOffset(days=1))\n",
    "                if not data.empty:  \n",
    "                    merged_df.loc[idx, f'Close Price Day {day}'] = data['Close'][0] \n",
    "    \n",
    "    # Calculate Returns and Highest Day Return\n",
    "    for day in DESIRED_DAYS: \n",
    "        merged_df[f'Return ({day} Days)'] = ((merged_df[f'Close Price Day {day}'] - merged_df['Close Price Day 1']) / merged_df['Close Price Day 1']) * 100\n",
    "        \n",
    "    merged_df['Highest Day Return'] = merged_df[[f'Return ({day} Days)' for day in [5, 7]]].max(axis=1)\n",
    "    merged_df['Highest Close Price'] = merged_df[[f'Close Price Day {day}' for day in desired_days]].max(axis=1)\n",
    "\n",
    "    for col in merged_df.columns:\n",
    "        if 'Close Price' in col or 'Return' in col:\n",
    "            merged_df[col] = merged_df[col].round(2)\n",
    "\n",
    "    merged_df.dropna(inplace=True)\n",
    "    merged_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    merged_df['Short Squeeze'] = 0\n",
    "    merged_df.loc[merged_df['Highest Day Return'] >= 20, 'Short Squeeze'] = 1\n",
    "    merged_df.loc[merged_df['Highest Day Return'] < 20, 'Short Squeeze'] = 0\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518804e0-ef6f-41d9-a7d9-b1951cd362d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "merged_df = feature_engineering(short_df, insider_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b35f9b-c678-4371-83de-929131f3fdff",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eaf40f-a582-4c0c-bd53-8c6639de6bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Short Squeeze\n",
    "short_squeeze_df = merged_df[merged_df['Short Squeeze'] == 1]\n",
    "for idx, row in short_squeeze_df.iterrows():\n",
    "    symbol = row['Symbol']\n",
    "    data = merged_df.loc[idx]\n",
    "    data = data.sort_values('Date')\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    days_to_plot = DESIRED_DAYS\n",
    "    closing_prices = [data[f'Close Price Day {day}'].iloc[-1] for day in days_to_plot]\n",
    "    plt.plot(days_to_plot, closing_prices, 'o', color='blue')\n",
    "    highest_closing_price = max(closing_prices)\n",
    "    highest_closing_price_day = days_to_plot[closing_prices.index(highest_closing_price)]\n",
    "    plt.plot(highest_closing_price_day, highest_closing_price, 'ro')\n",
    "    plt.annotate('Highest Closing Price',\n",
    "                 xy=(highest_closing_price_day, highest_closing_price),\n",
    "                 xytext=(highest_closing_price_day + 0.5, highest_closing_price),\n",
    "                 arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
    "                 fontsize=8,\n",
    "                 ha='left')\n",
    "    plt.xlabel('Day')\n",
    "    plt.ylabel('Closing Price')\n",
    "    plt.title(f'Short Squeeze: {symbol}')\n",
    "    plt.xticks(days_to_plot, [f'Day {day}' for day in days_to_plot])\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092770ab-503e-4e74-b5f6-2f88c67fb9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at each short squeeze company 30 days before and after Insider Trading activity occurs\n",
    "for symbol, data in grouped_df:\n",
    "    start_date = pd.to_datetime(data['Date'].min()) - pd.DateOffset(days=30)\n",
    "    end_date = pd.to_datetime(data['Date'].max()) + pd.DateOffset(days=30)\n",
    "    yf_data = yf.download(symbol, start=start_date, end=end_date)\n",
    "    close_prices = yf_data['Close']\n",
    "    plot = close_prices.hvplot(title=f'{symbol} Short Squeeze', ylabel='Closing Price').opts(width=600, height=400)\n",
    "    display(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70ee5d7-336c-4eac-883a-cfb9264e395e",
   "metadata": {},
   "source": [
    "### 5. One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "90ffbefb-b2d0-4945-ac33-8ed06d5430e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df):\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    categorical_variables = ['Company Name', 'Sector', 'Industry']\n",
    "    encoded_df = pd.DataFrame(encoder.fit_transform(df[categorical_variables]))\n",
    "    encoded_df.columns = encoder.get_feature_names_out(categorical_variables)\n",
    "    numerical_df = df.drop(columns=categorical_variables, axis=1)\n",
    "    return pd.concat([numerical_df, encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6244d34b-2520-403e-abf8-b922146f798f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kyle/opt/anaconda3/envs/dev/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# One hot encode the data\n",
    "merged_df = one_hot_encode(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a30bdda-3b7e-4442-87c2-2a25df929b38",
   "metadata": {},
   "source": [
    "### 6. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "14f3482b-a1de-49ec-832d-3d26a3ff83c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_df.drop(columns=['Short Squeeze'])\n",
    "y = merged_df['Short Squeeze']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e97530b-412e-42b3-b7d9-70cec7721374",
   "metadata": {},
   "source": [
    "### 7. Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f22b3632-5e3e-4876-9797-c4094b7a5339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'Symbol' column from X_train and X_test\n",
    "X_train_scaled = X_train.drop('Symbol', axis=1)\n",
    "X_test_scaled = X_test.drop('Symbol', axis=1)\n",
    "\n",
    "# Drop the 'Date' column from X_train and X_test\n",
    "X_train_scaled = X_train_scaled.drop('Date', axis=1)\n",
    "X_test_scaled = X_test_scaled.drop('Date', axis=1)\n",
    "\n",
    "# Scale the numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_scaled)\n",
    "X_test_scaled = scaler.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc6387c-dc72-4bc8-bb05-8c2ccfc0fb2e",
   "metadata": {},
   "source": [
    "### 8. Model Training\n",
    "#### Feedforward Neural Network (FNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "16d4ce2e-a325-4859-840e-2f9c9b7b96c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3/3 - 0s - loss: 0.6614 - accuracy: 0.5753 - 336ms/epoch - 112ms/step\n",
      "Epoch 2/50\n",
      "3/3 - 0s - loss: 0.5763 - accuracy: 0.7671 - 5ms/epoch - 2ms/step\n",
      "Epoch 3/50\n",
      "3/3 - 0s - loss: 0.5128 - accuracy: 0.8493 - 4ms/epoch - 1ms/step\n",
      "Epoch 4/50\n",
      "3/3 - 0s - loss: 0.4622 - accuracy: 0.8493 - 4ms/epoch - 1ms/step\n",
      "Epoch 5/50\n",
      "3/3 - 0s - loss: 0.4184 - accuracy: 0.8493 - 3ms/epoch - 1ms/step\n",
      "Epoch 6/50\n",
      "3/3 - 0s - loss: 0.3818 - accuracy: 0.8493 - 5ms/epoch - 2ms/step\n",
      "Epoch 7/50\n",
      "3/3 - 0s - loss: 0.3511 - accuracy: 0.8767 - 4ms/epoch - 1ms/step\n",
      "Epoch 8/50\n",
      "3/3 - 0s - loss: 0.3235 - accuracy: 0.8904 - 4ms/epoch - 1ms/step\n",
      "Epoch 9/50\n",
      "3/3 - 0s - loss: 0.2987 - accuracy: 0.9178 - 4ms/epoch - 1ms/step\n",
      "Epoch 10/50\n",
      "3/3 - 0s - loss: 0.2770 - accuracy: 0.9178 - 5ms/epoch - 2ms/step\n",
      "Epoch 11/50\n",
      "3/3 - 0s - loss: 0.2557 - accuracy: 0.9315 - 4ms/epoch - 1ms/step\n",
      "Epoch 12/50\n",
      "3/3 - 0s - loss: 0.2370 - accuracy: 0.9452 - 4ms/epoch - 1ms/step\n",
      "Epoch 13/50\n",
      "3/3 - 0s - loss: 0.2193 - accuracy: 0.9452 - 4ms/epoch - 1ms/step\n",
      "Epoch 14/50\n",
      "3/3 - 0s - loss: 0.2040 - accuracy: 0.9452 - 4ms/epoch - 1ms/step\n",
      "Epoch 15/50\n",
      "3/3 - 0s - loss: 0.1872 - accuracy: 0.9452 - 4ms/epoch - 1ms/step\n",
      "Epoch 16/50\n",
      "3/3 - 0s - loss: 0.1731 - accuracy: 0.9452 - 4ms/epoch - 1ms/step\n",
      "Epoch 17/50\n",
      "3/3 - 0s - loss: 0.1594 - accuracy: 0.9589 - 5ms/epoch - 2ms/step\n",
      "Epoch 18/50\n",
      "3/3 - 0s - loss: 0.1472 - accuracy: 0.9726 - 4ms/epoch - 1ms/step\n",
      "Epoch 19/50\n",
      "3/3 - 0s - loss: 0.1361 - accuracy: 0.9726 - 4ms/epoch - 1ms/step\n",
      "Epoch 20/50\n",
      "3/3 - 0s - loss: 0.1260 - accuracy: 0.9726 - 4ms/epoch - 1ms/step\n",
      "Epoch 21/50\n",
      "3/3 - 0s - loss: 0.1159 - accuracy: 0.9863 - 5ms/epoch - 2ms/step\n",
      "Epoch 22/50\n",
      "3/3 - 0s - loss: 0.1071 - accuracy: 0.9863 - 4ms/epoch - 1ms/step\n",
      "Epoch 23/50\n",
      "3/3 - 0s - loss: 0.0998 - accuracy: 0.9863 - 4ms/epoch - 1ms/step\n",
      "Epoch 24/50\n",
      "3/3 - 0s - loss: 0.0927 - accuracy: 1.0000 - 4ms/epoch - 1ms/step\n",
      "Epoch 25/50\n",
      "3/3 - 0s - loss: 0.0865 - accuracy: 1.0000 - 5ms/epoch - 2ms/step\n",
      "Epoch 26/50\n",
      "3/3 - 0s - loss: 0.0810 - accuracy: 1.0000 - 4ms/epoch - 1ms/step\n",
      "Epoch 27/50\n",
      "3/3 - 0s - loss: 0.0758 - accuracy: 1.0000 - 5ms/epoch - 2ms/step\n",
      "Epoch 28/50\n",
      "3/3 - 0s - loss: 0.0715 - accuracy: 1.0000 - 4ms/epoch - 1ms/step\n",
      "Epoch 29/50\n",
      "3/3 - 0s - loss: 0.0678 - accuracy: 1.0000 - 5ms/epoch - 2ms/step\n",
      "Epoch 30/50\n",
      "3/3 - 0s - loss: 0.0633 - accuracy: 1.0000 - 4ms/epoch - 1ms/step\n",
      "Epoch 31/50\n",
      "3/3 - 0s - loss: 0.0596 - accuracy: 1.0000 - 4ms/epoch - 1ms/step\n",
      "Epoch 32/50\n",
      "3/3 - 0s - loss: 0.0563 - accuracy: 1.0000 - 4ms/epoch - 1ms/step\n",
      "Epoch 33/50\n",
      "3/3 - 0s - loss: 0.0538 - accuracy: 1.0000 - 4ms/epoch - 1ms/step\n",
      "Epoch 34/50\n",
      "3/3 - 0s - loss: 0.0515 - accuracy: 1.0000 - 4ms/epoch - 1ms/step\n",
      "Epoch 35/50\n",
      "3/3 - 0s - loss: 0.0490 - accuracy: 1.0000 - 4ms/epoch - 1ms/step\n",
      "Epoch 36/50\n",
      "3/3 - 0s - loss: 0.0473 - accuracy: 1.0000 - 4ms/epoch - 1ms/step\n",
      "Epoch 37/50\n",
      "3/3 - 0s - loss: 0.0442 - accuracy: 1.0000 - 4ms/epoch - 1ms/step\n",
      "Epoch 38/50\n",
      "3/3 - 0s - loss: 0.0422 - accuracy: 1.0000 - 4ms/epoch - 1ms/step\n",
      "Epoch 39/50\n",
      "3/3 - 0s - loss: 0.0397 - accuracy: 1.0000 - 4ms/epoch - 1ms/step\n",
      "Epoch 40/50\n",
      "3/3 - 0s - loss: 0.0380 - accuracy: 1.0000 - 4ms/epoch - 1ms/step\n",
      "Epoch 41/50\n",
      "3/3 - 0s - loss: 0.0367 - accuracy: 1.0000 - 5ms/epoch - 2ms/step\n",
      "Epoch 42/50\n",
      "3/3 - 0s - loss: 0.0352 - accuracy: 1.0000 - 4ms/epoch - 1ms/step\n",
      "Epoch 43/50\n",
      "3/3 - 0s - loss: 0.0340 - accuracy: 1.0000 - 4ms/epoch - 1ms/step\n",
      "Epoch 44/50\n",
      "3/3 - 0s - loss: 0.0326 - accuracy: 1.0000 - 4ms/epoch - 1ms/step\n",
      "Epoch 45/50\n",
      "3/3 - 0s - loss: 0.0313 - accuracy: 1.0000 - 5ms/epoch - 2ms/step\n",
      "Epoch 46/50\n",
      "3/3 - 0s - loss: 0.0297 - accuracy: 1.0000 - 4ms/epoch - 1ms/step\n",
      "Epoch 47/50\n",
      "3/3 - 0s - loss: 0.0283 - accuracy: 1.0000 - 4ms/epoch - 1ms/step\n",
      "Epoch 48/50\n",
      "3/3 - 0s - loss: 0.0276 - accuracy: 1.0000 - 4ms/epoch - 1ms/step\n",
      "Epoch 49/50\n",
      "3/3 - 0s - loss: 0.0266 - accuracy: 1.0000 - 4ms/epoch - 1ms/step\n",
      "Epoch 50/50\n",
      "3/3 - 0s - loss: 0.0256 - accuracy: 1.0000 - 4ms/epoch - 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f874fe720b0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478a13c3-08a8-408d-9764-e2c23f18d561",
   "metadata": {},
   "source": [
    "### 9. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2db89a54-a2c4-4253-a824-a4e528c63d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 84.21%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f'Test Accuracy: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c1af90-05dc-4bf4-a472-c14f85163ee5",
   "metadata": {},
   "source": [
    "### 10. Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7ae278fc-d0f4-46d7-899a-a6a72e5c26e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 53ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test_scaled)\n",
    "# Since this is a binary classification problem, we'll set a threshold at 0.5\n",
    "predictions = [1 if pred > 0.5 else 0 for pred in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9607a9d9-ae20-4013-af8e-58d633da15d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Actual  Predicted\n",
      "4        1          0\n",
      "67       0          0\n",
      "40       0          0\n",
      "28       0          0\n",
      "39       0          0\n",
      "55       0          0\n",
      "18       1          0\n",
      "12       0          1\n",
      "72       1          1\n",
      "83       1          1\n"
     ]
    }
   ],
   "source": [
    "# You might want to compare these predictions with the actual values\n",
    "comparison_df = pd.DataFrame({'Actual': y_test, 'Predicted': predictions})\n",
    "\n",
    "# Print out a sample of the comparison DataFrame\n",
    "print(comparison_df.sample(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c6007e-635d-4b28-ada9-1907d38a6b1e",
   "metadata": {},
   "source": [
    "### 11. Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d0b4c8d8-342a-4520-a46b-e73807680f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90        15\n",
      "           1       0.67      0.50      0.57         4\n",
      "\n",
      "    accuracy                           0.84        19\n",
      "   macro avg       0.77      0.72      0.74        19\n",
      "weighted avg       0.83      0.84      0.83        19\n",
      "\n",
      "[[14  1]\n",
      " [ 2  2]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2b9b298c-4b7c-4819-a1d4-b3dddebfb790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: fnn_model/assets\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save('fnn_model')\n",
    "\n",
    "# Load the model later\n",
    "from tensorflow.keras.models import load_model\n",
    "loaded_model = load_model('fnn_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7d1570-b481-48a0-a200-af4668e9a101",
   "metadata": {},
   "source": [
    "## Alternative Models\n",
    "### Random Forest vs SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8cf2c856-d6bc-41af-80a5-245e8756c96d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    61\n",
       "1    12\n",
       "Name: Short Squeeze, dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the distinct values in the original labels data\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7ca517cc-540a-465e-94dd-36835971ebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "\n",
    "# Fitting the model\n",
    "rf_model = rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Making predictions using the testing data\n",
    "rf_predictions = rf_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5eefb2f2-826d-42d2-88cc-97be3d3147b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate SMOTE\n",
    "smote_sampler = SMOTE(random_state=1, sampling_strategy='minority')\n",
    "\n",
    "# Fit the SMOTE model to the training data\n",
    "X_resampled, y_resampled = smote_sampler.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Fit the RandomForestClassifier on the resampled data\n",
    "model_resampled_rf = RandomForestClassifier()\n",
    "model_resampled_rf.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Generate predictions based on the resampled data model\n",
    "rf_resampled_predictions = model_resampled_rf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b1b2313c-7442-4729-8b9c-e82f73609d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "    Actual  Predicted\n",
      "40       0          0\n",
      "49       0          0\n",
      "55       0          0\n",
      "39       0          0\n",
      "35       0          0\n",
      "28       0          0\n",
      "22       0          0\n",
      "12       0          0\n",
      "72       1          1\n",
      "26       0          0\n",
      "Oversampled Data:\n",
      "    Actual  Predicted\n",
      "0        0          0\n",
      "72       1          1\n",
      "55       0          0\n",
      "35       0          0\n",
      "83       1          1\n",
      "18       1          1\n",
      "62       0          0\n",
      "12       0          0\n",
      "10       0          0\n",
      "44       0          0\n"
     ]
    }
   ],
   "source": [
    "comparison_rf = pd.DataFrame({'Actual': y_test, 'Predicted': rf_predictions})\n",
    "comparison_rf_resampled = pd.DataFrame({'Actual': y_test, 'Predicted': rf_resampled_predictions})\n",
    "print(f'Original Data:\\n{comparison_rf.sample(10)}')\n",
    "print(f'Oversampled Data:\\n{comparison_rf_resampled.sample(10)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cd6118f8-3290-47b6-9cb5-b52ca2860ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[15  0]\n",
      " [ 0  4]]\n",
      "Oversampled Data:\n",
      "[[15  0]\n",
      " [ 0  4]]\n"
     ]
    }
   ],
   "source": [
    "print(f'Original Data:\\n{confusion_matrix(y_test, rf_predictions)}')\n",
    "print(f'Oversampled Data:\\n{confusion_matrix(y_test, rf_resampled_predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "077119fd-8dab-46ba-a738-ca414db08fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data: 1.0\n",
      "Oversampled Data: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f'Original Data: {balanced_accuracy_score(y_test, rf_predictions)}')\n",
    "print(f'Oversampled Data: {balanced_accuracy_score(y_test, rf_resampled_predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3c46c739-130d-41de-915d-d0a6d1746614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       1.00      1.00      1.00      1.00      1.00      1.00        15\n",
      "          1       1.00      1.00      1.00      1.00      1.00      1.00         4\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1.00      1.00      1.00        19\n",
      "\n",
      "Oversampled Data:\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       1.00      1.00      1.00      1.00      1.00      1.00        15\n",
      "          1       1.00      1.00      1.00      1.00      1.00      1.00         4\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1.00      1.00      1.00        19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Original Data:\\n{classification_report_imbalanced(y_test, rf_predictions)}')\n",
    "print(f'Oversampled Data:\\n{classification_report_imbalanced(y_test, rf_resampled_predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dcfe10c2-401d-47ea-826a-2dba836a80de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['resampled_rf_model.joblib']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "dump(rf_model, 'rf_model.joblib')\n",
    "dump(model_resampled_rf, 'resampled_rf_model.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
