{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d747d1a4-8886-4542-9a9b-7a688ce24204",
   "metadata": {},
   "source": [
    "# Pedro - Short Queeze Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6fc2c9-97c4-4fd6-8bd8-2900aa31ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from pathlib import Path\n",
    "import pandas_market_calendars as mcal\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f87b9a-5895-41ac-8484-d83d60b3c992",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bf7205-d687-49fd-a908-7907b9be861a",
   "metadata": {},
   "source": [
    "### Short Interest Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d8f33b-19df-4275-be14-0fdd94dc6ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess short interest data\n",
    "short_df = pd.read_csv(Path(\"Resources/ShortFloat.csv\"))\n",
    "short_df.rename(columns={'ShortSqueeze.com Short Interest Data': 'Company Name'}, inplace=True)\n",
    "\n",
    "# Dropping irrelevant columns\n",
    "columns_to_drop = [\n",
    "    'Total Short Interest', 'Days to Cover', 'Performance (52-wk)', 'Short: Prior Mo', '% Change Mo/Mo',\n",
    "    'Shares: Float', 'Avg. Daily Vol.', 'Shares: Outstanding', 'Short Squeeze Rankingâ„¢', '% from 52-wk High',\n",
    "    '(abs)', '% from 200 day MA', '(abs).1', '% from 50 day MA', '(abs).2', '% Insider Ownership',\n",
    "    '% Institutional Ownership'\n",
    "]\n",
    "columns_to_drop = [col for col in columns_to_drop if col in short_df.columns]\n",
    "short_df.drop(columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Convert 'Short % of Float' and 'Market Cap' to numeric and apply filters\n",
    "short_df['Short % of Float'] = pd.to_numeric(short_df['Short % of Float'], errors='coerce')\n",
    "short_df = short_df[short_df['Short % of Float'] >= 17]\n",
    "short_df['Market Cap'] = pd.to_numeric(short_df['Market Cap'], errors='coerce')\n",
    "short_df = short_df[short_df['Market Cap'] >= 300000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685255fc-37da-446e-a4a6-8beb3ac5aa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the DataFrame\n",
    "short_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227726d8-d3a9-44c0-8458-c96e209d2415",
   "metadata": {},
   "source": [
    "### Insider Trading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81fbca0-e2a3-4fac-8a69-b4d2a5c0919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load insider trading data, remove dollar symbols, and convert columns to numeric\n",
    "insider_df = pd.read_csv(\"Resources/InsiderTrading.csv\")\n",
    "insider_df['Total Amount'] = insider_df['Total Amount'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n",
    "insider_df['Share Price'] = insider_df['Share Price'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n",
    "insider_df['Date'] = pd.to_datetime(insider_df['Date'])\n",
    "insider_df['Total Amount'] = pd.to_numeric(insider_df['Total Amount'], errors='coerce')\n",
    "insider_df = insider_df[insider_df['Total Amount'] >= 1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2aa217-204e-4902-ac04-ef8483e82277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the DataFrame\n",
    "insider_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120417d4-e74f-48bf-803d-57fb651bb5e0",
   "metadata": {},
   "source": [
    "## Date Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14f546b-0487-4b88-9511-059fa6c58620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Record Date' column to datetime, sort the dataframe by 'Record Date'.\n",
    "date_mapping = {\n",
    "    'JanA': '01-11', 'JanB': '01-25',\n",
    "    'FebA': '02-09', 'FebB': '02-27',\n",
    "    'MarA': '03-09', 'MarB': '03-24',\n",
    "    'AprA': '04-12', 'AprB': '04-25',\n",
    "    'MayA': '05-09', 'MayB': '05-24',\n",
    "    'JunA': '06-09', 'JunB': '06-27',\n",
    "    'JulA': '07-12', 'JulB': '07-25',\n",
    "    'AugA': '08-09', 'AugB': '08-24',\n",
    "    'SepA': '09-12', 'SepB': '09-26',\n",
    "    'OctA': '10-10', 'OctB': '10-24',\n",
    "    'NovA': '11-09', 'NovB': '11-27',\n",
    "    'DecA': '12-11', 'DecB': '12-27',\n",
    "}\n",
    "short_df['Record Date'] = pd.to_datetime(short_df['Record Date'].str.replace(r'(\\d{4})-(\\w+)', lambda m: f'{m.group(1)}-{date_mapping[m.group(2)]}'))\n",
    "short_df.sort_values('Record Date', inplace=True)\n",
    "short_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628db501-f3fb-4b60-b9b3-fa8c2406c4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge short_df and insider_df based on the Symbol column, rename columns, and select necessary columns\n",
    "merged_df = pd.merge(short_df, insider_df, on='Symbol')\n",
    "merged_df['Share Price'] = merged_df['Share Price'].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n",
    "merged_df = merged_df[['Symbol', 'Short % of Float', 'Total Amount', 'Record Date', 'Share Price', 'Company Name', 'Sector', 'Industry', 'Date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e255c0e1-9461-4465-a4fe-b442bae0dc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference between 'Date' and 'Record Date' for each row\n",
    "merged_df['Date_diff'] = (merged_df['Date'] - merged_df['Record Date']).dt.days\n",
    "\n",
    "# Filter out rows where 'Date_diff' is more than 30 and drop unnecessary columns\n",
    "merged_df = merged_df[merged_df['Date_diff'] >= 0]\n",
    "merged_df.sort_values(['Symbol', 'Date_diff'], inplace=True)\n",
    "merged_df.drop_duplicates(subset=['Symbol', 'Date'], keep='first', inplace=True)\n",
    "merged_df = merged_df[merged_df['Date_diff'] <= 30]\n",
    "merged_df.drop(columns=['Record Date', 'Date_diff'], inplace=True)\n",
    "\n",
    "# Reorder columns\n",
    "new_column_order = ['Symbol', 'Short % of Float', 'Total Amount', 'Date', 'Company Name', 'Sector', 'Industry']\n",
    "merged_df = merged_df[new_column_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0215c50c-a59f-42c1-b9c1-efc2f35e8e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the DataFrame\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832b0961-99a7-41cc-99f8-e3531729d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns for Close Prices at future dates and calculate Returns.\n",
    "nyse = mcal.get_calendar('NYSE')\n",
    "desired_days = [1, 2, 3, 4, 5, 7]\n",
    "for day in desired_days:\n",
    "    merged_df[f'Close Price Day {day}'] = np.nan\n",
    "\n",
    "for idx, row in merged_df.iterrows():\n",
    "    trading_days = nyse.valid_days(start_date=row['Date'], end_date=row['Date'] + pd.DateOffset(days=10))\n",
    "\n",
    "    for day in desired_days:\n",
    "        if day <= len(trading_days):\n",
    "            data = yf.download(row['Symbol'], start=trading_days[day - 1], end=trading_days[day - 1] + pd.DateOffset(days=1))\n",
    "            if not data.empty:  \n",
    "                merged_df.loc[idx, f'Close Price Day {day}'] = data['Close'][0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fca0c28-2ca6-4052-87cb-83672571855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Returns and Highest Day Return\n",
    "for day in [5, 7]: \n",
    "    merged_df[f'Return ({day} Days)'] = ((merged_df[f'Close Price Day {day}'] - merged_df['Close Price Day 1']) / merged_df['Close Price Day 1']) * 100\n",
    "\n",
    "merged_df['Highest Day Return'] = merged_df[[f'Return ({day} Days)' for day in [5, 7]]].max(axis=1)\n",
    "merged_df['Highest Close Price'] = merged_df[[f'Close Price Day {day}' for day in desired_days]].max(axis=1)\n",
    "\n",
    "# Format the 'Close Price' and 'Return' columns to have 2 decimal places\n",
    "for col in merged_df.columns:\n",
    "    if 'Close Price' in col or 'Return' in col:\n",
    "        merged_df[col] = merged_df[col].round(2)\n",
    "\n",
    "# Drop NaNs and reset the index\n",
    "merged_df.dropna(inplace=True)\n",
    "merged_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f8ee54-df77-4d2a-aa25-3d3833852534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the DataFrame\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd506853-9ef6-413a-bfc0-86d330d50228",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Short Squeeze'] = 0\n",
    "merged_df.loc[merged_df['Highest Day Return'] >= 20, 'Short Squeeze'] = 1\n",
    "merged_df.loc[merged_df['Highest Day Return'] < 20, 'Short Squeeze'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a282c8-9ff9-4140-b10e-33a32f5b7178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the DataFrame\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834f3cb6-9529-4ab6-b08d-c9bab745552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review the data types associated with the columns\n",
    "merged_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7f8b9e-abe2-411a-9613-0c5b11555994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of categorical variables \n",
    "categorical_variables = list(merged_df.select_dtypes(\"object\").columns)\n",
    "\n",
    "# Display the categorical variables list\n",
    "display(categorical_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19da2e54-b52c-4fd3-ab0c-10b0771ece25",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f677db-e207-4073-b084-6940f5832e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the categorical variables using OneHotEncoder\n",
    "encoded_data = enc.fit_transform(merged_df[categorical_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a063e6e-dc8d-44ed-9d44-00d8d4e51991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the encoded variables\n",
    "encoded_df = pd.DataFrame(encoded_data, columns = enc.get_feature_names(categorical_variables))\n",
    "\n",
    "# Review the DataFrame\n",
    "encoded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87d931a-85ff-494e-8abd-cfd6e926df9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the numerical variables from the original DataFrame to the one-hot encoding DataFrame\n",
    "encoded_df = pd.concat([merged_df.select_dtypes([\"int64\", \"float64\"]), encoded_df], axis=1)\n",
    "\n",
    "# Review the DataFrame\n",
    "encoded_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede137ea-5fac-45e8-97bd-b8f198604106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment the features from the target\n",
    "X = encoded_df.drop(columns='Short Squeeze')\n",
    "y = encoded_df['Short Squeeze']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c73619-c997-45ec-8d78-ce1d56e90d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665ad650-0125-4454-9dd4-760e4f0a82e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the features training dataset\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the features data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11ef694-b9f9-41db-a73b-182d12e60ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
